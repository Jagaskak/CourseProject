{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ITMTF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GToajN2-aXJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef950eec-fa41-435e-cf7d-63876828c726"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkFkgUC4YICe",
        "outputId": "6934ee11-ee90-483b-a400-d54f16c56c3b"
      },
      "source": [
        "!pip install pyLDAvis\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "from gensim import models\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models import Phrases\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import statsmodels\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.17.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.15)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.4)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (50.3.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "iuJ711r8YLYc",
        "outputId": "f47648c9-4702-4ef2-a189-9793c5e81377"
      },
      "source": [
        "'''\n",
        "This cell contains function definitions.\n",
        "'''\n",
        "\n",
        "def rel_purity(currPurity, prevPurity):\n",
        "    return abs(currPurity - prevPurity)/prevPurity\n",
        "\n",
        "\n",
        "# Select the model and print the topics\n",
        "def get_topics(lda_model, num_topics=-1, num_words=100, prob_thresh=0.8):\n",
        "    topics = []\n",
        "    for topic, topic_words in lda_model.print_topics(num_topics=num_topics, num_words=num_words):\n",
        "        words = topic_words.split(\" + \")\n",
        "        all_words = []\n",
        "        all_prob = 0\n",
        "        for elem in words:\n",
        "            prob, word = elem.split(\"*\")\n",
        "            all_prob += float(prob)\n",
        "            all_words.append(word.split('\"')[1])\n",
        "\n",
        "            if all_prob >= prob_thresh:\n",
        "                break\n",
        "        topics.append((topic, all_words))\n",
        "    \n",
        "    return topics\n",
        "\n",
        "\n",
        "'''\n",
        "Significant portion of grangers_causality_matrix function was taken from stackoverflow post:\n",
        "https://stackoverflow.com/questions/58005681/is-it-possible-to-run-a-vector-autoregression-analysis-on-a-large-gdp-data-with\n",
        "'''\n",
        "def grangers_causality_matrix(data, variables, maxlag=5, test='ssr_ftest', verbose=False):\n",
        "    dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    lags    = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    \n",
        "    for c in dataset.columns:\n",
        "        for r in dataset.index:            \n",
        "            test_result = grangercausalitytests(data[[r,c]], maxlag=maxlag, verbose=False)\n",
        "            p_values = [round(test_result[i+1][0][test][1], 4) for i in range(maxlag)]\n",
        "            \n",
        "            if verbose: \n",
        "                print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "\n",
        "            # smaller p-val corresponds to higher f-val\n",
        "            min_p_value_i = np.argmin(p_values)\n",
        "            min_p_value = p_values[min_p_value_i]\n",
        "            dataset.loc[r, c] = min_p_value\n",
        "            \n",
        "            lags.loc[r, c] = min_p_value_i\n",
        "   \n",
        "    return dataset, lags\n",
        "\n",
        "def get_causal_vars(data, significance=0.95, getLags=False, getCausalSig=False, verbose=False):\n",
        "    cols = data.columns[:-1]\n",
        "    causal_vars = []\n",
        "    causal_lags = []\n",
        "    \n",
        "    for col in cols:\n",
        "        try:\n",
        "            gc, lags = grangers_causality_matrix(data[[col, 'LastPrice']], \n",
        "                                             variables=[col, 'LastPrice'], \n",
        "                                             verbose=False)\n",
        "        except:\n",
        "            raise Exception(data[[col, 'LastPrice']])\n",
        "        \n",
        "        gc = 1 - gc\n",
        "        \n",
        "        col_causes = gc.loc['LastPrice', col] >= significance\n",
        "        col_causedBy = gc.loc[col, 'LastPrice'] >= significance\n",
        "        if col_causes or col_causedBy:\n",
        "            if getCausalSig:\n",
        "                causal_vars.append((col, max(gc.loc['LastPrice', col], gc.loc[col, 'LastPrice'])))\n",
        "            else:\n",
        "                causal_vars.append(col)\n",
        "            \n",
        "            if getLags:\n",
        "                # if sig. granger causality for topic causing ts and ts causing topic, choose whichever is higher\n",
        "                if col_causes and col_causedBy:\n",
        "                    if gc.loc['LastPrice', col] >= gc.loc[col, 'LastPrice']:\n",
        "                        causal_lags.append(lags.loc['LastPrice', col])\n",
        "                    else:\n",
        "                        causal_lags.append(lags.loc[col, 'LastPrice'] * -1)\n",
        "                elif col_causes:\n",
        "                    causal_lags.append(lags.loc['LastPrice', col])\n",
        "                else:\n",
        "                    causal_lags.append(lags.loc[col, 'LastPrice'] * -1)\n",
        "    if getLags:\n",
        "        return causal_vars, causal_lags\n",
        "    \n",
        "    return causal_vars\n",
        "                \n",
        "def get_word_stream(nytimes, topics, causal_topics):\n",
        "    ct_ws = []\n",
        "    for ct in causal_topics:\n",
        "        causal_vocab = list(set(topics[ct][1]))\n",
        "        date_terms = pd.DataFrame(np.zeros((len(unique_dates), len(causal_vocab))), index=unique_dates, columns=causal_vocab)\n",
        "        \n",
        "        for word in causal_vocab:\n",
        "            date_terms[word] = date_term_cnts[word]\n",
        "            \n",
        "        ct_ws.append((ct, date_terms))\n",
        "    \n",
        "    return ct_ws\n",
        "\n",
        "def get_impact_words(topic_wordstream, significance=0.95, verbose=False):\n",
        "    topic_impact_words = []\n",
        "    \n",
        "    first = True\n",
        "    for topic, ws in topic_wordstream:\n",
        "        ws_prices = ws.join(stock_prices.set_index('Date')).dropna()        \n",
        "        ws_gc = get_causal_vars(ws_prices, significance=significance, getCausalSig=True, verbose=verbose)\n",
        "        \n",
        "        pos = []\n",
        "        neg = []\n",
        "        for word, sig in ws_gc:                \n",
        "            corr = pearsonr(ws_prices[word], stock_prices['LastPrice'])[0]\n",
        "            if corr > 0:\n",
        "                pos.append((word, sig))\n",
        "            else:\n",
        "                neg.append((word, sig))\n",
        "                \n",
        "        topic_impact_words.append((topic, pos, neg))\n",
        "    \n",
        "    return topic_impact_words\n",
        "        \n",
        "def construct_prior(impact_words, curr_k, sig=0.95, alter_k=True):\n",
        "    # find number of topics that we are splitting\n",
        "    if alter_k:\n",
        "        new_k = curr_k + len(impact_words)\n",
        "    else:\n",
        "        new_k = curr_k\n",
        "    word_priors = np.zeros((new_k, date_term_cnts.shape[1])) + (1/len(id2word))\n",
        "\n",
        "    i = 0\n",
        "    for num, pos, neg in impact_words:\n",
        "        pos_denom = sum([granger-sig for word, granger in pos])\n",
        "        neg_denom = sum([granger-sig for word, granger in neg])\n",
        "        \n",
        "        if len(pos) < 0.1 * len(neg):\n",
        "            # num neg words >> num pos\n",
        "            for word, granger in pos:              \n",
        "                word_priors[i, id2word.token2id[word]] = 0\n",
        "            for word, granger in neg:\n",
        "                word_priors[i, id2word.token2id[word]] = (granger-sig)/neg_denom \n",
        "            \n",
        "        elif len(neg) < 0.1 * len(pos):\n",
        "            # num pos words >> num neg\n",
        "            for word, granger in pos:              \n",
        "                word_priors[i, id2word.token2id[word]] = (granger-sig)/pos_denom \n",
        "            for word, granger in neg:\n",
        "                word_priors[i, id2word.token2id[word]] = 0\n",
        "            \n",
        "\n",
        "        for word, granger in pos:              \n",
        "            word_priors[i, id2word.token2id[word]] = (granger-sig)/pos_denom \n",
        "        \n",
        "        for word, granger in neg:\n",
        "            word_priors[i + 1, id2word.token2id[word]] = (granger-sig)/neg_denom \n",
        "        \n",
        "        i += 2\n",
        "    \n",
        "    # normalize matrix\n",
        "#     row_sums = word_priors.sum(axis=1, keepdims=True)\n",
        "#     return word_priors/row_sums\n",
        "    return word_priors\n",
        "            \n",
        "def calculate_purity(pWords, nWords):\n",
        "    n = float(len(pWords) + len(nWords))\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    pProb = len(pWords)/n\n",
        "    nProb = len(nWords)/n\n",
        "    \n",
        "    pProb = pProb if pProb else 1\n",
        "    nProb = nProb if nProb else 1\n",
        "    \n",
        "    entropy = pProb * np.log2(pProb) + nProb * np.log2(nProb)\n",
        "    purity = 100 + 100 * entropy\n",
        "    return purity\n",
        "\n",
        "def show_plot(x, yData, xaxislabel, yaxislabel, labels, saveAs=None):\n",
        "    if len(yData) != len(labels):\n",
        "        raise ValueError(\"Number of labels should equal number of lines you want to plot\")\n",
        "    \n",
        "    plt.xlabel(xaxislabel)\n",
        "    plt.ylabel(yaxislabel)\n",
        "    plt.grid()\n",
        "        \n",
        "    for i in range(len(yData)):\n",
        "        plt.plot(x, yData[i], label=labels[i])\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
        "    \n",
        "    if saveAs:\n",
        "        plt.savefig(saveAs)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Timestamp('2000-05-01 05:00:00+0000', tz='UTC'), Timestamp('2000-05-02 05:00:00+0000', tz='UTC'), Timestamp('2000-05-03 05:00:00+0000', tz='UTC'), Timestamp('2000-05-04 05:00:00+0000', tz='UTC'), Timestamp('2000-05-05 05:00:00+0000', tz='UTC'), Timestamp('2000-05-06 05:00:00+0000', tz='UTC'), Timestamp('2000-05-07 05:00:00+0000', tz='UTC'), Timestamp('2000-05-08 05:00:00+0000', tz='UTC'), Timestamp('2000-05-09 05:00:00+0000', tz='UTC'), Timestamp('2000-05-10 05:00:00+0000', tz='UTC'), Timestamp('2000-05-11 05:00:00+0000', tz='UTC'), Timestamp('2000-05-12 05:00:00+0000', tz='UTC'), Timestamp('2000-05-13 05:00:00+0000', tz='UTC'), Timestamp('2000-05-14 05:00:00+0000', tz='UTC'), Timestamp('2000-05-15 05:00:00+0000', tz='UTC'), Timestamp('2000-05-16 05:00:00+0000', tz='UTC'), Timestamp('2000-05-17 05:00:00+0000', tz='UTC'), Timestamp('2000-05-18 05:00:00+0000', tz='UTC'), Timestamp('2000-05-19 05:00:00+0000', tz='UTC'), Timestamp('2000-05-20 05:00:00+0000', tz='UTC'), Timestamp('2000-05-21 05:00:00+0000', tz='UTC'), Timestamp('2000-05-22 05:00:00+0000', tz='UTC'), Timestamp('2000-05-23 05:00:00+0000', tz='UTC'), Timestamp('2000-05-24 05:00:00+0000', tz='UTC'), Timestamp('2000-05-25 05:00:00+0000', tz='UTC'), Timestamp('2000-05-26 05:00:00+0000', tz='UTC'), Timestamp('2000-05-27 05:00:00+0000', tz='UTC'), Timestamp('2000-05-28 05:00:00+0000', tz='UTC'), Timestamp('2000-05-29 05:00:00+0000', tz='UTC'), Timestamp('2000-05-30 05:00:00+0000', tz='UTC'), Timestamp('2000-05-31 05:00:00+0000', tz='UTC'), Timestamp('2000-06-01 05:00:00+0000', tz='UTC'), Timestamp('2000-06-02 05:00:00+0000', tz='UTC'), Timestamp('2000-06-03 05:00:00+0000', tz='UTC'), Timestamp('2000-06-04 05:00:00+0000', tz='UTC'), Timestamp('2000-06-05 05:00:00+0000', tz='UTC'), Timestamp('2000-06-06 05:00:00+0000', tz='UTC'), Timestamp('2000-06-07 05:00:00+0000', tz='UTC'), Timestamp('2000-06-08 05:00:00+0000', tz='UTC'), Timestamp('2000-06-09 05:00:00+0000', tz='UTC'), Timestamp('2000-06-10 05:00:00+0000', tz='UTC'), Timestamp('2000-06-11 05:00:00+0000', tz='UTC'), Timestamp('2000-06-12 05:00:00+0000', tz='UTC'), Timestamp('2000-06-13 05:00:00+0000', tz='UTC'), Timestamp('2000-06-14 05:00:00+0000', tz='UTC'), Timestamp('2000-06-15 05:00:00+0000', tz='UTC'), Timestamp('2000-06-16 05:00:00+0000', tz='UTC'), Timestamp('2000-06-17 05:00:00+0000', tz='UTC'), Timestamp('2000-06-18 05:00:00+0000', tz='UTC'), Timestamp('2000-06-19 05:00:00+0000', tz='UTC'), Timestamp('2000-06-20 05:00:00+0000', tz='UTC'), Timestamp('2000-06-21 05:00:00+0000', tz='UTC'), Timestamp('2000-06-22 05:00:00+0000', tz='UTC'), Timestamp('2000-06-23 05:00:00+0000', tz='UTC'), Timestamp('2000-06-24 05:00:00+0000', tz='UTC'), Timestamp('2000-06-25 05:00:00+0000', tz='UTC'), Timestamp('2000-06-26 05:00:00+0000', tz='UTC'), Timestamp('2000-06-27 05:00:00+0000', tz='UTC'), Timestamp('2000-06-28 05:00:00+0000', tz='UTC'), Timestamp('2000-06-29 05:00:00+0000', tz='UTC'), Timestamp('2000-06-30 05:00:00+0000', tz='UTC'), Timestamp('2000-07-01 05:00:00+0000', tz='UTC'), Timestamp('2000-07-02 05:00:00+0000', tz='UTC'), Timestamp('2000-07-03 05:00:00+0000', tz='UTC'), Timestamp('2000-07-04 05:00:00+0000', tz='UTC'), Timestamp('2000-07-05 05:00:00+0000', tz='UTC'), Timestamp('2000-07-06 05:00:00+0000', tz='UTC'), Timestamp('2000-07-07 05:00:00+0000', tz='UTC'), Timestamp('2000-07-08 05:00:00+0000', tz='UTC'), Timestamp('2000-07-09 05:00:00+0000', tz='UTC'), Timestamp('2000-07-10 05:00:00+0000', tz='UTC'), Timestamp('2000-07-11 05:00:00+0000', tz='UTC'), Timestamp('2000-07-12 05:00:00+0000', tz='UTC'), Timestamp('2000-07-13 05:00:00+0000', tz='UTC'), Timestamp('2000-07-14 05:00:00+0000', tz='UTC'), Timestamp('2000-07-15 05:00:00+0000', tz='UTC'), Timestamp('2000-07-16 05:00:00+0000', tz='UTC'), Timestamp('2000-07-17 05:00:00+0000', tz='UTC'), Timestamp('2000-07-18 05:00:00+0000', tz='UTC'), Timestamp('2000-07-19 05:00:00+0000', tz='UTC'), Timestamp('2000-07-20 05:00:00+0000', tz='UTC'), Timestamp('2000-07-21 05:00:00+0000', tz='UTC'), Timestamp('2000-07-22 05:00:00+0000', tz='UTC'), Timestamp('2000-07-23 05:00:00+0000', tz='UTC'), Timestamp('2000-07-24 05:00:00+0000', tz='UTC'), Timestamp('2000-07-25 05:00:00+0000', tz='UTC'), Timestamp('2000-07-26 05:00:00+0000', tz='UTC'), Timestamp('2000-07-27 05:00:00+0000', tz='UTC'), Timestamp('2000-07-28 05:00:00+0000', tz='UTC'), Timestamp('2000-07-29 05:00:00+0000', tz='UTC'), Timestamp('2000-07-30 05:00:00+0000', tz='UTC'), Timestamp('2000-07-31 05:00:00+0000', tz='UTC'), Timestamp('2000-08-01 05:00:00+0000', tz='UTC'), Timestamp('2000-08-02 05:00:00+0000', tz='UTC'), Timestamp('2000-08-03 05:00:00+0000', tz='UTC'), Timestamp('2000-08-04 05:00:00+0000', tz='UTC'), Timestamp('2000-08-05 05:00:00+0000', tz='UTC'), Timestamp('2000-08-06 05:00:00+0000', tz='UTC'), Timestamp('2000-08-07 05:00:00+0000', tz='UTC'), Timestamp('2000-08-08 05:00:00+0000', tz='UTC'), Timestamp('2000-08-09 05:00:00+0000', tz='UTC'), Timestamp('2000-08-10 05:00:00+0000', tz='UTC'), Timestamp('2000-08-11 05:00:00+0000', tz='UTC'), Timestamp('2000-08-12 05:00:00+0000', tz='UTC'), Timestamp('2000-08-13 05:00:00+0000', tz='UTC'), Timestamp('2000-08-14 05:00:00+0000', tz='UTC'), Timestamp('2000-08-15 05:00:00+0000', tz='UTC'), Timestamp('2000-08-16 05:00:00+0000', tz='UTC'), Timestamp('2000-08-17 05:00:00+0000', tz='UTC'), Timestamp('2000-08-18 05:00:00+0000', tz='UTC'), Timestamp('2000-08-19 05:00:00+0000', tz='UTC'), Timestamp('2000-08-20 05:00:00+0000', tz='UTC'), Timestamp('2000-08-21 05:00:00+0000', tz='UTC'), Timestamp('2000-08-22 05:00:00+0000', tz='UTC'), Timestamp('2000-08-23 05:00:00+0000', tz='UTC'), Timestamp('2000-08-24 05:00:00+0000', tz='UTC'), Timestamp('2000-08-25 05:00:00+0000', tz='UTC'), Timestamp('2000-08-26 05:00:00+0000', tz='UTC'), Timestamp('2000-08-27 05:00:00+0000', tz='UTC'), Timestamp('2000-08-28 05:00:00+0000', tz='UTC'), Timestamp('2000-08-29 05:00:00+0000', tz='UTC'), Timestamp('2000-08-30 05:00:00+0000', tz='UTC'), Timestamp('2000-08-31 05:00:00+0000', tz='UTC'), Timestamp('2000-09-01 05:00:00+0000', tz='UTC'), Timestamp('2000-09-02 05:00:00+0000', tz='UTC'), Timestamp('2000-09-03 05:00:00+0000', tz='UTC'), Timestamp('2000-09-04 05:00:00+0000', tz='UTC'), Timestamp('2000-09-05 05:00:00+0000', tz='UTC'), Timestamp('2000-09-06 05:00:00+0000', tz='UTC'), Timestamp('2000-09-07 05:00:00+0000', tz='UTC'), Timestamp('2000-09-08 05:00:00+0000', tz='UTC'), Timestamp('2000-09-09 05:00:00+0000', tz='UTC'), Timestamp('2000-09-10 05:00:00+0000', tz='UTC'), Timestamp('2000-09-11 05:00:00+0000', tz='UTC'), Timestamp('2000-09-12 05:00:00+0000', tz='UTC'), Timestamp('2000-09-13 05:00:00+0000', tz='UTC'), Timestamp('2000-09-14 05:00:00+0000', tz='UTC'), Timestamp('2000-09-15 05:00:00+0000', tz='UTC'), Timestamp('2000-09-16 05:00:00+0000', tz='UTC'), Timestamp('2000-09-17 05:00:00+0000', tz='UTC'), Timestamp('2000-09-18 05:00:00+0000', tz='UTC'), Timestamp('2000-09-19 05:00:00+0000', tz='UTC'), Timestamp('2000-09-20 05:00:00+0000', tz='UTC'), Timestamp('2000-09-21 05:00:00+0000', tz='UTC'), Timestamp('2000-09-22 05:00:00+0000', tz='UTC'), Timestamp('2000-09-23 05:00:00+0000', tz='UTC'), Timestamp('2000-09-24 05:00:00+0000', tz='UTC'), Timestamp('2000-09-25 05:00:00+0000', tz='UTC'), Timestamp('2000-09-26 05:00:00+0000', tz='UTC'), Timestamp('2000-09-27 05:00:00+0000', tz='UTC'), Timestamp('2000-09-28 05:00:00+0000', tz='UTC'), Timestamp('2000-09-29 05:00:00+0000', tz='UTC'), Timestamp('2000-09-30 05:00:00+0000', tz='UTC'), Timestamp('2000-10-01 05:00:00+0000', tz='UTC'), Timestamp('2000-10-02 05:00:00+0000', tz='UTC'), Timestamp('2000-10-03 05:00:00+0000', tz='UTC'), Timestamp('2000-10-04 05:00:00+0000', tz='UTC'), Timestamp('2000-10-05 05:00:00+0000', tz='UTC'), Timestamp('2000-10-06 05:00:00+0000', tz='UTC'), Timestamp('2000-10-07 05:00:00+0000', tz='UTC'), Timestamp('2000-10-08 05:00:00+0000', tz='UTC'), Timestamp('2000-10-09 05:00:00+0000', tz='UTC'), Timestamp('2000-10-10 05:00:00+0000', tz='UTC'), Timestamp('2000-10-11 05:00:00+0000', tz='UTC'), Timestamp('2000-10-12 05:00:00+0000', tz='UTC'), Timestamp('2000-10-13 05:00:00+0000', tz='UTC'), Timestamp('2000-10-14 05:00:00+0000', tz='UTC'), Timestamp('2000-10-15 05:00:00+0000', tz='UTC'), Timestamp('2000-10-16 05:00:00+0000', tz='UTC'), Timestamp('2000-10-17 05:00:00+0000', tz='UTC'), Timestamp('2000-10-18 05:00:00+0000', tz='UTC'), Timestamp('2000-10-19 05:00:00+0000', tz='UTC'), Timestamp('2000-10-20 05:00:00+0000', tz='UTC'), Timestamp('2000-10-21 05:00:00+0000', tz='UTC'), Timestamp('2000-10-22 05:00:00+0000', tz='UTC'), Timestamp('2000-10-23 05:00:00+0000', tz='UTC'), Timestamp('2000-10-24 05:00:00+0000', tz='UTC'), Timestamp('2000-10-25 05:00:00+0000', tz='UTC'), Timestamp('2000-10-26 05:00:00+0000', tz='UTC'), Timestamp('2000-10-27 05:00:00+0000', tz='UTC'), Timestamp('2000-10-28 05:00:00+0000', tz='UTC'), Timestamp('2000-10-29 05:00:00+0000', tz='UTC'), Timestamp('2000-10-30 05:00:00+0000', tz='UTC'), Timestamp('2000-10-31 05:00:00+0000', tz='UTC'), Timestamp('2000-11-01 05:00:00+0000', tz='UTC')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-05-03 05:00:00+00:00</td>\n",
              "      <td>[two, years, ago, homer, bush, came, yankee, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-05-02 05:00:00+00:00</td>\n",
              "      <td>[texas, record, tell, op, ed, april, paul, bur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-05-01 05:00:00+00:00</td>\n",
              "      <td>[top, foreign, policy, adviser, gov, george, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-05-03 05:00:00+00:00</td>\n",
              "      <td>[aides, gov, george, bush, fought, back, today...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-05-03 05:00:00+00:00</td>\n",
              "      <td>[gov, tommy, thompson, wisconsin, named, chair...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5801</th>\n",
              "      <td>2000-10-31 05:00:00+00:00</td>\n",
              "      <td>[new, york, times, cbs, news, poll, var, strin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5802</th>\n",
              "      <td>2000-10-31 05:00:00+00:00</td>\n",
              "      <td>[tick, tock, diner, ted, friedrich, stockbroke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5803</th>\n",
              "      <td>2000-11-01 05:00:00+00:00</td>\n",
              "      <td>[difference, us, vital, issue, would, go, wash...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5804</th>\n",
              "      <td>2000-11-01 05:00:00+00:00</td>\n",
              "      <td>[bush, administration, wanted, overturn, would...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5805</th>\n",
              "      <td>2000-11-01 05:00:00+00:00</td>\n",
              "      <td>[first, gov, jeb, bush, florida, told, hallowe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5806 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                          date                                            content\n",
              "0    2000-05-03 05:00:00+00:00  [two, years, ago, homer, bush, came, yankee, b...\n",
              "1    2000-05-02 05:00:00+00:00  [texas, record, tell, op, ed, april, paul, bur...\n",
              "2    2000-05-01 05:00:00+00:00  [top, foreign, policy, adviser, gov, george, b...\n",
              "3    2000-05-03 05:00:00+00:00  [aides, gov, george, bush, fought, back, today...\n",
              "4    2000-05-03 05:00:00+00:00  [gov, tommy, thompson, wisconsin, named, chair...\n",
              "...                        ...                                                ...\n",
              "5801 2000-10-31 05:00:00+00:00  [new, york, times, cbs, news, poll, var, strin...\n",
              "5802 2000-10-31 05:00:00+00:00  [tick, tock, diner, ted, friedrich, stockbroke...\n",
              "5803 2000-11-01 05:00:00+00:00  [difference, us, vital, issue, would, go, wash...\n",
              "5804 2000-11-01 05:00:00+00:00  [bush, administration, wanted, overturn, would...\n",
              "5805 2000-11-01 05:00:00+00:00  [first, gov, jeb, bush, florida, told, hallowe...\n",
              "\n",
              "[5806 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0fJqMhVZKt1",
        "outputId": "9a57a360-54b2-409c-eb14-b694a8e9c92b"
      },
      "source": [
        "def ITMTF(threshold=10**-16, mu=1, k=30, alter_k=False, constructTNGraph=False)\n",
        "    isBaseline = True\n",
        "    baselineLDAModel = None\n",
        "    baseline_purity = 0.0\n",
        "    prevPurity = 100\n",
        "    purity = 0\n",
        "    threshold = 10**-16\n",
        "    mu = 1\n",
        "    k = 30\n",
        "    alpha = \"auto\"\n",
        "    eta = \"auto\"\n",
        "    alter_k = False\n",
        "    constructTNGraph = False\n",
        "\n",
        "    all_avg_purities = []\n",
        "    all_avg_conf = []\n",
        "\n",
        "    # for k in [10, 20, 30, 40, 10]:\n",
        "    for mu in [0.51, 0.6, 0.7, 0.8, 0.9, 1]:\n",
        "        print (\"\\n\\nk: \", k)\n",
        "        print (\"mu: \", mu)\n",
        "\n",
        "        avg_purities = []\n",
        "        avg_confidences = []\n",
        "\n",
        "        num_iter = 0\n",
        "        while num_iter < 5:\n",
        "            if isBaseline:\n",
        "                baselineLDAModel = gensim.models.ldamodel.LdaModel(\n",
        "                                        corpus=tfidf_corpus,\n",
        "                                        id2word=id2word,\n",
        "                                        num_topics=k, \n",
        "                                        alpha='auto',  # assuming that topic distribution is assymetric. Not all topics equally represented in corpus.\n",
        "                                        eta='auto')        \n",
        "\n",
        "                topics = get_topics(baselineLDAModel, prob_thresh=0.3, num_words=50)\n",
        "                ct_ws = get_word_stream(nytimes, topics, [i for i in range(k)])\n",
        "\n",
        "                impact_words = get_impact_words(ct_ws)\n",
        "\n",
        "                purities = [calculate_purity(topic[1], topic[2]) for topic in impact_words]\n",
        "                baseline_purity = sum(purities)/len(purities)\n",
        "                prev_purity = baseline_purity\n",
        "\n",
        "                print (\"Baseline Purity: \", baseline_purity)\n",
        "\n",
        "                isBaseline = False\n",
        "    #             avg_purities.append(baseline_purity)\n",
        "                continue\n",
        "            else:\n",
        "                print (\"\\nNumber topics: \", k)          \n",
        "\n",
        "                lda_model = gensim.models.ldamodel.LdaModel(\n",
        "                                        corpus=tfidf_corpus,\n",
        "                                        id2word=id2word,\n",
        "                                        num_topics=k, \n",
        "                                        alpha=alpha,  # assuming that topic distribution is assymetric. Not all topics equally represented in corpus.\n",
        "                                        eta=eta,\n",
        "                                        decay=mu)\n",
        "\n",
        "            topics = get_topics(lda_model, prob_thresh=0.3, num_words=50)\n",
        "            document_topics = lda_model.get_document_topics(corpus)\n",
        "            date_doc_topics = list(zip(nytimes[\"Date\"], lda_model.get_document_topics(corpus)))\n",
        "\n",
        "            # for any given day, you look at all the diff topics and identify the prob of that topic\n",
        "            date_topic_prob = np.zeros((len(unique_dates), k))\n",
        "            for date, article in date_doc_topics:\n",
        "                i = unique_dates.index(date)\n",
        "                for topic, prob in article:\n",
        "                    date_topic_prob[i][topic] += prob \n",
        "\n",
        "            date_topic = pd.DataFrame(date_topic_prob, index=unique_dates)\n",
        "            date_topic[\"Date\"] = unique_dates\n",
        "\n",
        "            date_topic_prices = date_topic.set_index('Date').join(stock_prices.set_index('Date')).dropna()\n",
        "            causal_topics, ct_lags = get_causal_vars(date_topic_prices, getLags=True)\n",
        "            ct_ws = get_word_stream(nytimes, topics, causal_topics)\n",
        "\n",
        "            impact_words = get_impact_words(ct_ws)\n",
        "\n",
        "            # Calculate Purity\n",
        "            purities = [calculate_purity(topic[1], topic[2]) for topic in impact_words]\n",
        "            if len(purities) == 0:\n",
        "                # Try again\n",
        "                continue\n",
        "                avg_purity= 0\n",
        "            else:\n",
        "                avg_purity = sum(purities)/len(purities)\n",
        "\n",
        "            prevPurity = purity\n",
        "            purity = avg_purity\n",
        "\n",
        "            avg_purities.append(purity)\n",
        "            print (\"Purity: \", purity)  \n",
        "\n",
        "            # Calculate Confidence\n",
        "            all_conf = 0.0\n",
        "            num_words = 0\n",
        "            for topic, pos, neg in impact_words:\n",
        "                num_words += len(pos) + len(neg)\n",
        "                for words, gc in pos:\n",
        "                    all_conf += gc\n",
        "\n",
        "                for words, gc in neg:\n",
        "                    all_conf += gc\n",
        "\n",
        "            avg_confidences.append(all_conf/num_words*100 if num_words else 0)\n",
        "            print (\"Avg. Conf: \", all_conf/num_words*100 if num_words else 0)\n",
        "\n",
        "\n",
        "            # Prior for next iteration\n",
        "            eta = construct_prior(impact_words, k, alter_k=alter_k)\n",
        "\n",
        "            # adjust num topics\n",
        "            if alter_k:\n",
        "                k += len(impact_words)\n",
        "\n",
        "            num_iter += 1\n",
        "\n",
        "        all_avg_purities.append(avg_purities)\n",
        "        all_avg_conf.append(avg_confidences)\n",
        "\n",
        "        # reset params\n",
        "        isBaseline = True\n",
        "        eta = \"auto\"\n",
        "\n",
        "        if constructTNGraph:\n",
        "            if k == 40:\n",
        "                alter_k=True\n",
        "    \n",
        "    return lda_model, baselineLDAModel, all_avg_purities, all_avg_conf\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('ago', 0.07712049873418031),\n",
              "  ('awesome', 0.23220574510227418),\n",
              "  ('backup', 0.2198823985449398),\n",
              "  ('backups', 0.2515408271170864),\n",
              "  ('bases', 0.19264069440348208),\n",
              "  ('bellinger', 0.27548950382241366),\n",
              "  ('bench', 0.1896343958919212),\n",
              "  ('bush', 0.007894722475376273),\n",
              "  ('came', 0.08612993720379283),\n",
              "  ('catcher', 0.26148042600790294),\n",
              "  ('clay', 0.2135830725972484),\n",
              "  ('games', 0.1562902360625982),\n",
              "  ('girardi', 0.27548950382241366),\n",
              "  ('homer', 0.21658937110880933),\n",
              "  ('jim', 0.1245222966630543),\n",
              "  ('joe', 0.1146922085996351),\n",
              "  ('leyritz', 0.27548950382241366),\n",
              "  ('speed', 0.17969479700110466),\n",
              "  ('stole', 0.20587332073042908),\n",
              "  ('strength', 0.13402729061444735),\n",
              "  ('turner', 0.2108175460504276),\n",
              "  ('two', 0.04788545375938528),\n",
              "  ('versatility', 0.27548950382241366),\n",
              "  ('whose', 0.0887458288821732),\n",
              "  ('yankee', 0.20825706839694694),\n",
              "  ('yankees', 0.19264069440348208),\n",
              "  ('years', 0.05159983565074285)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRBH0GV0W19W"
      },
      "source": [
        "# k_labels = [\"10\", \"20\", \"30\", \"40\", \"varying tn\"]\n",
        "# show_plot(range(1, 7), all_avg_purities,    \"Iteration\", \"Average Purity\", k_labels, saveAs=\"purity_varying_tn_2.png\")\n",
        "# show_plot(range(1, 7), all_avg_conf, \"Iteration\", \"Average Confidence\", k_labels, saveAs=\"confidence_varying_tn_1.png\")\n",
        "\n",
        "mu_labels = [\"0.51\", \"0.6\", \"0.7\", \"0.8\", \"0.9\", \"1\"]\n",
        "show_plot(range(1, 6), np.array(all_avg_purities[5:]), \"Iteration\", \"Average Purity\", mu_labels, saveAs=\"purity_mu.png\")\n",
        "show_plot(range(1, 6), np.array(all_avg_conf[5:]), \"Iteration\", \"Average Confidence\", mu_labels, saveAs=\"confidence_mu.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvKyC24KbFtV"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(baselineLDAModel, corpus, id2word)\n",
        "pyLDAvis.save_html(vis, 'baselineLDA_mu.html')\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcq6hyYcJdsF"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "pyLDAvis.save_html(vis, 'lda_mu.html')\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}